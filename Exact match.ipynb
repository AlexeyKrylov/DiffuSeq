{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a53009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bd82153",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/SparQL/spec-english_train_split.txt', 'r') as f:\n",
    "    data = [len(i.replace('\\n', ' ').split(' ')) for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2fd4505b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "347b2642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('select distinct ?SUBJ_1 , ?OBJ_3 where { ?SUBJ_1 wdt:P31 OBJ_2 . ?SUBJ_1 rdfs:label ?OBJ_3 . filter ( contains ( lcase ( ?OBJ_3 ) , STR_VALUE_1 ) ) . filter ( lang ( ?OBJ_3 ) = STR_VALUE_2 ) . } limit NUM_VALUE_1'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe540f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./checkpoint-path/TestT5-v2/ema_0.999_010000.pt.samples/seed105_step0.json', 'r') as json_file:\n",
    "#     json_list = list(json_file)\n",
    "with open('./checkpoint-path/T5-base/model012000.samples/seed105_step0.json', 'r') as json_file:\n",
    "    json_list = list(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f364cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'pred': [json.loads(i)[\"recover\"] for i in json_list], 'target': [json.loads(i)[\"reference\"] for i in json_list], 'source': [json.loads(i)[\"source\"] for i in json_list]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddd16020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8cfbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.iloc[:576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f317dd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['pred'] == data['target']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "874ecc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['pred'] == data['target']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "009d2022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.75"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'][data['pred'] != data['target']].apply(lambda x : len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1abb75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.59375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred'][data['pred'] != data['target']].apply(lambda x : len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75db6f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS] select?SUBJ_1 where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 wdt:P2051?OBJ_3. } order by desc (?OBJ_3 ) limit NUM_VALUE_1 [SEP]',\n",
       "       '[CLS] select distinct?SUBJ_1 where {?SUBJ_1 wdt:P272 OBJ_2.?SUBJ_1 wdt:P31 OBJ_3. } [SEP]',\n",
       "       '[CLS] select distinct?SUBJ_1,?OBJ_3 where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 rdfs:label?OBJ_3. filter ( contains ( lcase (?OBJ_3 ), STR_VALUE_1 ) ). filter ( lang (?OBJ_3 ) = STR_VALUE_2 ). } limit NUM_VALUE_1 [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 p:P166?OBJ_2.?SUBJ_2 ps:P166?OBJ_3.?SUBJ_2 pq:P585?OBJ_4. filter ( contains ( year (?OBJ_4 ), STR_VALUE_1 ) ). } [SEP]',\n",
       "       '[CLS] select distinct?SUBJ_2 where { SUBJ_1 wdt:P156?OBJ_2.?SUBJ_2 wdt:P31 OBJ_3. } [SEP]',\n",
       "       '[CLS] select?SUBJ_1 where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 wdt:P1436?OBJ_3.?SUBJ_1 wdt:P31 OBJ_4. } order by desc (?OBJ_3 ) limit NUM_VALUE_1 [SEP]',\n",
       "       '[CLS] select?OBJ_4 where { SUBJ_1 p:P39?OBJ_2.?SUBJ_2 ps:P39 OBJ_3.?SUBJ_2 pq:P1365?OBJ_4. } [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 wdt:P463?OBJ_2.?SUBJ_2 wdt:P488?OBJ_3. } [SEP]',\n",
       "       '[CLS] select?SUBJ_1 where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 wdt:P2051?OBJ_3. } order by desc (?OBJ_3 ) limit NUM_VALUE_1 [SEP]',\n",
       "       '[CLS] select distinct?SUBJ_1 where {?SUBJ_1 wdt:P272 OBJ_2.?SUBJ_1 wdt:P31 OBJ_3. } [SEP]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'][data['pred'] != data['target']].iloc[:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52779b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS] select?OBJ_3 where { SUBJ_1 wdt:P264?OBJ_2.?SUBJ_2 wdt:P355?OBJ_3. } [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 p:P2293?OBJ_2.?SUBJ_2 wdt:P1376?OBJ_3. } } [SEP]',\n",
       "       '[CLS] select distinct where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 wdt:P2160 OBJ_3... } order [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 wdt:P20?OBJ_2.?SUBJ_2 ps:P2293?OBJ_3. }?SUBJ_2 pq:P459',\n",
       "       '[CLS] [CLS] select distinct where where {?SUBJ_1 wdt:P31 OBJ_2.?SUBJ_1 rdfs:label?OBJ_3. } order by (?OBJ_3 ) ). }',\n",
       "       '[CLS] select distinct where { SUBJ_1 p:P2293?OBJ_2?OBJ_2.?SUBJ_2 ps:P39 OBJ_3. } [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 wdt:P264?OBJ_2.?SUBJ_2 ps:P2293?OBJ_3?OBJ_3. } [SEP]',\n",
       "       '[CLS] [CLS] select?OBJ_3 where { SUBJ_1 wdt:P264?OBJ_2.?SUBJ_2 wdt:P710?OBJ_3. } order [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 wdt:P264.?SUBJ_2 wdt:P355?OBJ_3. } [SEP] [SEP]',\n",
       "       '[CLS] select?OBJ_3 where { SUBJ_1 wdt:P27?OBJ_2?OBJ_2..?SUBJ_2 pq:P518?OBJ_3. } [SEP]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred'][data['pred'] != data['target']].iloc[-10:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1085d831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.666666666666666"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'][data['pred'] == data['target']].apply(lambda x : len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d51e6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/SparQL/src-english_train_split.txt', 'r', encoding='utf8') as f:\n",
    "    src = f.read()\n",
    "with open('./datasets/SparQL/src-english_dev_split.txt', 'r', encoding='utf8') as f:\n",
    "    src_1 = f.read()\n",
    "with open('./datasets/SparQL/src-english_test_split.txt', 'r', encoding='utf8') as f:\n",
    "    src_2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e44910d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33347"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(src.replace('\\n', ' ').replace('?', '').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e8e46013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17699"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(src_1.replace('\\n', ' ').replace('?', '').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30e43596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17677"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(src_2.replace('\\n', ' ').replace('?', '').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cc80a72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4733"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(src_1.replace('\\n', ' ').replace('?', '').split()) - set(src.replace('\\n', ' ').replace('?', '').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70598bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(src_2.replace('\\n', ' ').replace('?', '').split()) - set(src.replace('\\n', ' ').replace('?', '').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3aaa7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d88429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bba7f1a9053408ca3b34e1aba62e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krilo\\anaconda3\\envs\\DiffuSeq_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\krilo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using a model of type deberta-v2 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6b1119b03a423bb95dc7bd83b7e3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing T5ForConditionalGeneration: ['deberta.encoder.layer.7.output.dense.weight', 'mask_predictions.classifier.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.key_proj.bias', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.3.output.dense.bias', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.3.intermediate.dense.weight', 'deberta.encoder.layer.0.attention.self.value_proj.bias', 'deberta.encoder.layer.0.output.dense.weight', 'deberta.encoder.layer.5.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.dense.weight', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.output.dense.bias', 'mask_predictions.LayerNorm.bias', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.output.dense.weight', 'deberta.encoder.layer.5.output.dense.bias', 'deberta.encoder.layer.3.attention.self.query_proj.bias', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.1.intermediate.dense.weight', 'deberta.encoder.layer.9.attention.self.value_proj.weight', 'mask_predictions.LayerNorm.weight', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.1.intermediate.dense.bias', 'deberta.encoder.layer.4.attention.self.query_proj.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.self.query_proj.bias', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.1.output.dense.bias', 'deberta.encoder.layer.9.output.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.dense.weight', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.5.attention.output.dense.weight', 'mask_predictions.classifier.weight', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.0.intermediate.dense.weight', 'deberta.encoder.layer.10.attention.self.query_proj.bias', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.2.output.dense.bias', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.dense.weight', 'mask_predictions.dense.weight', 'deberta.encoder.layer.1.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.output.dense.bias', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.dense.bias', 'deberta.encoder.layer.4.attention.self.key_proj.bias', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.5.intermediate.dense.bias', 'deberta.encoder.layer.5.attention.self.key_proj.bias', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'mask_predictions.dense.bias', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.5.output.dense.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.2.output.dense.weight', 'deberta.encoder.layer.4.output.dense.bias', 'deberta.encoder.layer.3.intermediate.dense.bias', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.LayerNorm.weight', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.0.output.dense.bias', 'deberta.encoder.layer.6.attention.self.query_proj.bias', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.bias', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.self.query_proj.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.bias', 'deberta.encoder.layer.5.attention.self.value_proj.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.self.key_proj.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.10.attention.self.value_proj.bias', 'lm_predictions.lm_head.bias', 'deberta.encoder.layer.0.attention.output.dense.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.self.value_proj.bias', 'deberta.embeddings.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.10.attention.self.key_proj.bias', 'deberta.encoder.layer.3.output.dense.weight', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.self.value_proj.bias', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.4.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.self.query_proj.bias', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.4.intermediate.dense.bias', 'deberta.encoder.layer.9.attention.self.key_proj.bias', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.5.intermediate.dense.weight', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.2.intermediate.dense.bias', 'deberta.encoder.layer.8.attention.self.key_proj.bias', 'deberta.encoder.layer.7.attention.self.value_proj.bias', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.self.value_proj.bias', 'deberta.encoder.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.5.attention.output.dense.bias', 'deberta.encoder.rel_embeddings.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.key_proj.bias', 'lm_predictions.lm_head.dense.bias', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.value_proj.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.embeddings.word_embeddings.weight', 'deberta.encoder.layer.3.attention.output.dense.bias', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.0.intermediate.dense.bias', 'deberta.encoder.layer.3.attention.output.dense.weight', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.encoder.layer.7.attention.self.query_proj.bias', 'deberta.encoder.layer.10.output.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.encoder.layer.2.intermediate.dense.weight', 'deberta.encoder.layer.3.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.bias', 'deberta.encoder.layer.0.attention.self.key_proj.bias', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.self.value_proj.bias', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.4.intermediate.dense.weight', 'deberta.encoder.layer.6.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['decoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'shared.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.final_layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8600b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "\n",
    "model1 = T5Model.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81567604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8310e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.Tensor(8, 512, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99d0f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7ec15bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd252cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaModel(\n",
       "  (embeddings): DebertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
       "    (LayerNorm): DebertaLayerNorm()\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (encoder): DebertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaLayer(\n",
       "        (attention): DebertaAttention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (pos_dropout): StableDropout()\n",
       "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "          (output): DebertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): DebertaLayerNorm()\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(1024, 768)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17355f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  5 13:02:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P0    25W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94c15f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(128100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=128100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8dfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23763b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 16 14:00:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P0    28W /  N/A |   1514MiB /  6144MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     25992      C   ...s\\DiffuSeq_env\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d11e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ -0.7539,   0.5977,  -2.4375,  ...,   1.2500,  -0.7891,   3.5156],\n",
       "        [ 11.3750,  -4.8750,   9.0625,  ...,   4.8438,  14.3750,  -5.7812],\n",
       "        [-16.6250,  11.0625, -20.8750,  ...,  10.6875,  22.2500,  25.0000],\n",
       "        ...,\n",
       "        [  2.2344,   6.7500, -11.0625,  ..., -11.3125,  13.5625,  16.6250],\n",
       "        [  4.2500,   5.1250, -12.2500,  ..., -11.9375,  13.5000,  17.0000],\n",
       "        [  4.0625,   6.9688, -12.2500,  ..., -11.3750,  11.9375,  16.6250]],\n",
       "       device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2ea2c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krilo\\anaconda3\\envs\\DiffuSeq_env\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:293: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(torch.float32, torch.Size([256])), (torch.float32, torch.Size([256])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([512])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([262144])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([1048576])), (torch.float32, torch.Size([16449536]))]\n",
      "131\n",
      "{torch.float32}\n"
     ]
    }
   ],
   "source": [
    "# prints currently alive Tensors and Variables\n",
    "import torch\n",
    "import gc\n",
    "ll = list()\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            ll.append((obj.dtype, obj.flatten().size()))\n",
    "    except:\n",
    "        pass\n",
    "print(sorted(ll, key=lambda x : x[1]))\n",
    "print(len(ll))\n",
    "print(set([i[0] for i in ll]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4687973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints currently alive Tensors and Variables\n",
    "import torch\n",
    "import gc\n",
    "ll = list()\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)) and obj.dtype == 'torch.float32':\n",
    "            print(obj.dtype, obj.flatten().size())\n",
    "            del obj\n",
    "    except:\n",
    "        pass\n",
    "print(sorted(ll, key=lambda x : x[1]))\n",
    "print(len(ll))\n",
    "print(set([i[0] for i in ll]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
